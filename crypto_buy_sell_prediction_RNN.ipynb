{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "midpoint between "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T22:11:27.069593Z",
     "start_time": "2018-09-08T22:11:27.045657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTING BITCOIN HISTORICAL DATA FROM 7/16/2010-9/1/2018\n",
      "BITCOIN data has 2938 input dates with 7 features \n",
      " \n",
      "RESCALING THE DATA\n",
      "RESHAPED ORIGINAL DATA FROM (2938, 1) TRAIN DATA INTO THE  (2878, 60, 1) SHAPE FOR TEST DATA\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd \n",
    "# Import dataset(s) from trusted source. In this analysis prices given by Yahoo! Finanace will be used for its integrity.\n",
    "\n",
    "print('IMPORTING BITCOIN HISTORICAL DATA FROM 7/16/2010-9/1/2018')\n",
    "btc_train_set= pd.read_csv('BTC-USD_train.csv')\n",
    "\n",
    "print('BITCOIN data has {} input dates with {} features \\n '.format(btc_train_set.shape[0],btc_train_set.shape[1]))\n",
    "# btc.drop(['Date','Open','High','Low','Close','Adj Close'], axis = 1)\n",
    "btc_train = btc_train_set.iloc[:,1:2].values\n",
    "\n",
    "# Feature Scaling/Creating data structure with 60 timesteps and 1 output\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "print('RESCALING THE DATA')\n",
    "sc = MinMaxScaler()\n",
    "btc = sc.fit_transform(btc_train)\n",
    "# Create the inputs and the ouputs\n",
    "X_train_btc = []\n",
    "y_train_btc = []\n",
    "for i in range(60,2938):\n",
    "    X_train_btc.append(btc[i-60:i,0])\n",
    "    y_train_btc.append(btc[i,0])\n",
    "X_train_btc,y_train_btc = np.array(X_train_btc),np.array(y_train_btc)   \n",
    "# Getting the inputs and the ouputs\n",
    "# X_train_btc = btc[0:1825]\n",
    "# y_train_btc = btc[1:1826]\n",
    "\n",
    "\n",
    "# Reshaping the data given the new dimensionality to reshape numpy array\n",
    "X_train_btc = np.reshape(X_train_btc, (X_train_btc.shape[0],X_train_btc.shape[1],1))\n",
    "print('RESHAPED ORIGINAL DATA FROM {} TRAIN DATA INTO THE  {} SHAPE FOR TEST DATA'.format(btc_train.shape,X_train_btc.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T21:41:32.052679Z",
     "start_time": "2018-09-08T21:41:32.020765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTING BITCOIN CASH HISTORICAL DATA FROM 7/31/2017-9/1/2018\n",
      "BITCOIN CASH data has 381 input dates with 7 features \n",
      " \n",
      "RESCALING THE DATA\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "print('IMPORTING BITCOIN CASH HISTORICAL DATA FROM 7/31/2017-9/1/2018')\n",
    "bch = pd.read_csv('BCH-USD_train.csv')\n",
    "bch_test = pd.read_csv('BCH-USD_train.csv')\n",
    "print('BITCOIN CASH data has {} input dates with {} features \\n '.format(bch.shape[0],bch.shape[1]))\n",
    "# bch.drop(['Date','Open','High','Low','Close','Adj Close'], axis = 1)\n",
    "# bch = bch.iloc[:,1:2].values\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# print('RESCALING THE DATA')\n",
    "# sc = MinMaxScaler()\n",
    "# bch = sc.fit_transform(bch)\n",
    "\n",
    "# X_train_bch = bch[0:380]\n",
    "# y_train_bch = bch[1:381]\n",
    "\n",
    "# X_train_bch = np.reshape(X_train_bch, (380, 1, 1))\n",
    "\n",
    "# print('finished')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T21:41:32.739147Z",
     "start_time": "2018-09-08T21:41:32.709203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTING ETHEREUM HISTORICAL DATA FROM 8/6/2015-9/1/2018\n",
      "ETHEREUM data has 1106 input dates with 7 features \n",
      " \n",
      "RESCALING THE DATA\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('IMPORTING ETHEREUM HISTORICAL DATA FROM 8/6/2015-9/1/2018')\n",
    "eth = pd.read_csv('ETH-USD_train.csv')\n",
    "print('ETHEREUM data has {} input dates with {} features \\n '.format(eth.shape[0],eth.shape[1]))\n",
    "# eth.drop(['Date','Open','High','Low','Close','Adj Close'], axis = 1)\n",
    "# eth = eth.iloc[:,1:2].values\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# print('RESCALING THE DATA')\n",
    "# sc = MinMaxScaler()\n",
    "# eth = sc.fit_transform(eth)\n",
    "\n",
    "# X_train_eth = eth[0:1105]\n",
    "# y_train_eth = eth[1:1106]\n",
    "\n",
    "# X_train_eth = np.reshape(X_train_eth, (1105, 1, 1))\n",
    "\n",
    "# print('finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T21:41:33.407198Z",
     "start_time": "2018-09-08T21:41:33.357342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTING LITECOIN HISTORICAL DATA FROM 10/23/2013-9/1/2018\n",
      "LITECOIN data has 1758 input dates with 7 features  \n",
      " \n",
      "RESCALING THE DATA\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "print('IMPORTING LITECOIN HISTORICAL DATA FROM 10/23/2013-9/1/2018')\n",
    "ltc = pd.read_csv('LTC-USD_train.csv')\n",
    "print('LITECOIN data has {} input dates with {} features  \\n '.format(ltc.shape[0],ltc.shape[1]))\n",
    "# # ltc.drop(['Date','Open','High','Low','Close','Adj Close'], axis = 1)\n",
    "# ltc = ltc.iloc[:,1:2].values\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# print('RESCALING THE DATA')\n",
    "# sc = MinMaxScaler()\n",
    "# ltc = sc.fit_transform(ltc)\n",
    "\n",
    "# X_train_ltc = ltc[0:1157]\n",
    "# y_train_ltc = ltc[1:1158]\n",
    "\n",
    "# X_train_ltc = np.reshape(X_train_ltc, (1157, 1, 1))\n",
    "# print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T21:41:34.178337Z",
     "start_time": "2018-09-08T21:41:34.149383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTING RIPPLE HISTORICAL DATA FROM 1/20/2015-9/1/2018\n",
      "RIPPLE data has 1304 input dates with 7 features  \n",
      " \n",
      "RESCALING THE DATA\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "print('IMPORTING RIPPLE HISTORICAL DATA FROM 1/20/2015-9/1/2018')\n",
    "xrp = pd.read_csv('XRP-USD_train.csv')\n",
    "print('RIPPLE data has {} input dates with {} features  \\n '.format(xrp.shape[0],xrp.shape[1]))\n",
    "# xrp.drop(['Date','Open','High','Low','Close','Adj Close'], axis = 1)\n",
    "# xrp = xrp.iloc[:,1:2].values\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# print('RESCALING THE DATA')\n",
    "# sc = MinMaxScaler()\n",
    "# xrp = sc.fit_transform(xrp)\n",
    "\n",
    "# X_train_xrp = xrp[0:1303]\n",
    "# y_train_xrp= xrp[1:1304]\n",
    "\n",
    "# X_train_xrp = np.reshape(X_train_xrp, (1303, 1, 1))\n",
    "# print('finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T22:11:33.985811Z",
     "start_time": "2018-09-08T22:11:33.277691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_30 (LSTM)               (None, 60, 72)            21312     \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 60, 72)            0         \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 60, 72)            41760     \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 60, 72)            0         \n",
      "_________________________________________________________________\n",
      "lstm_32 (LSTM)               (None, 60, 72)            41760     \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 60, 72)            0         \n",
      "_________________________________________________________________\n",
      "lstm_33 (LSTM)               (None, 72)                41760     \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 73        \n",
      "=================================================================\n",
      "Total params: 146,665\n",
      "Trainable params: 146,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Part 2 - Building the RNN\n",
    "# Importing the Keras libraries and packages necessary for making the stacked Lstm with dropout regularization \n",
    "# to help prevent overfitting\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# Initialising the RNN\n",
    "#Regression is used for its continuous outputs instead of categorical \n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the input layer and the LSTM layers\n",
    "# units = number of LSTM cells\n",
    "#Keep units high to gather more dimensions in that data, return_sequences=True for stacked RNN\n",
    "#input shape = timesteps, indicators \n",
    "#No sigmoid/relu/etc... activation function\n",
    "regressor.add(LSTM(units = 72, input_shape = (X_train_btc.shape[1],1),return_sequences=True))\n",
    "regressor.add(Dropout(.2))\n",
    "\n",
    "regressor.add(LSTM(units= 72,return_sequences=True))\n",
    "regressor.add(Dropout(.2))\n",
    "\n",
    "regressor.add(LSTM(units=72,return_sequences=True))\n",
    "regressor.add(Dropout(.2))\n",
    "\n",
    "# remove return_sequences..No more sequences being returned\n",
    "regressor.add(LSTM(units=72))\n",
    "regressor.add(Dropout(.2))\n",
    "\n",
    "# regressor.add(Dense(1,activation = 'softmax')) Not classification, i.e. no softmax\n",
    "# Adding the output layer (t+1)\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "regressor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T22:24:26.470532Z",
     "start_time": "2018-09-08T22:11:35.191200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the RNN\n",
      "Fitting the RNN to the Training set\n",
      "Training The Data\n",
      "Epoch 1/110\n",
      "2878/2878 [==============================] - 11s 4ms/step - loss: 0.0037\n",
      "Epoch 2/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 0.0018\n",
      "Epoch 3/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 0.0013\n",
      "Epoch 4/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 0.0012\n",
      "Epoch 5/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 0.0011\n",
      "Epoch 6/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 8.4906e-04\n",
      "Epoch 7/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 0.0011\n",
      "Epoch 8/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 8.4055e-04\n",
      "Epoch 9/110\n",
      "2878/2878 [==============================] - 7s 3ms/step - loss: 8.8590e-04\n",
      "Epoch 10/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 7.7966e-04\n",
      "Epoch 11/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 8.0188e-04\n",
      "Epoch 12/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 6.7095e-04\n",
      "Epoch 13/110\n",
      "2878/2878 [==============================] - 7s 3ms/step - loss: 7.0254e-04\n",
      "Epoch 14/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 6.6220e-04\n",
      "Epoch 15/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 5.7831e-04\n",
      "Epoch 16/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 7.6438e-04\n",
      "Epoch 17/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 5.8353e-04\n",
      "Epoch 18/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 7.3024e-04\n",
      "Epoch 19/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 7.5191e-04\n",
      "Epoch 20/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 5.8589e-04\n",
      "Epoch 21/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 6.6654e-04\n",
      "Epoch 22/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 5.0814e-04\n",
      "Epoch 23/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 5.6518e-04\n",
      "Epoch 24/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 5.6292e-04\n",
      "Epoch 25/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 6.5856e-04\n",
      "Epoch 26/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 6.1365e-04\n",
      "Epoch 27/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 5.9957e-04\n",
      "Epoch 28/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 5.3685e-04\n",
      "Epoch 29/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 5.4789e-04\n",
      "Epoch 30/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.6937e-04\n",
      "Epoch 31/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 5.0127e-04\n",
      "Epoch 32/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.2790e-04\n",
      "Epoch 33/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.9915e-04\n",
      "Epoch 34/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 5.1070e-04\n",
      "Epoch 35/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 5.4323e-04\n",
      "Epoch 36/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.2905e-04\n",
      "Epoch 37/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 5.9888e-04\n",
      "Epoch 38/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.3393e-04\n",
      "Epoch 39/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.7664e-04\n",
      "Epoch 40/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.1505e-04\n",
      "Epoch 41/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.8488e-04\n",
      "Epoch 42/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.6334e-04\n",
      "Epoch 43/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.5783e-04\n",
      "Epoch 44/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.0331e-04\n",
      "Epoch 45/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.7468e-04\n",
      "Epoch 46/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.5510e-04\n",
      "Epoch 47/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.0093e-04\n",
      "Epoch 48/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.4092e-04\n",
      "Epoch 49/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 5.7149e-04\n",
      "Epoch 50/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 5.5986e-04\n",
      "Epoch 51/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.0175e-04\n",
      "Epoch 52/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.3591e-04\n",
      "Epoch 53/110\n",
      "2878/2878 [==============================] - 7s 3ms/step - loss: 4.7119e-04\n",
      "Epoch 54/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.4397e-04\n",
      "Epoch 55/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.7869e-04\n",
      "Epoch 56/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.2535e-04\n",
      "Epoch 57/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.6339e-04\n",
      "Epoch 58/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.6275e-04\n",
      "Epoch 59/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.7944e-04\n",
      "Epoch 60/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.1756e-04\n",
      "Epoch 61/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.1305e-04\n",
      "Epoch 62/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.2471e-04\n",
      "Epoch 63/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.9846e-04\n",
      "Epoch 64/110\n",
      "2878/2878 [==============================] - 7s 3ms/step - loss: 4.3186e-04\n",
      "Epoch 65/110\n",
      "2878/2878 [==============================] - 7s 3ms/step - loss: 3.3260e-04\n",
      "Epoch 66/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.8006e-04\n",
      "Epoch 67/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.6238e-04\n",
      "Epoch 68/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.8974e-04\n",
      "Epoch 69/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.6611e-04\n",
      "Epoch 70/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.6618e-04\n",
      "Epoch 71/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.8304e-04\n",
      "Epoch 72/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.1548e-04\n",
      "Epoch 73/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.5188e-04\n",
      "Epoch 74/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.4785e-04\n",
      "Epoch 75/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.4679e-04\n",
      "Epoch 76/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.8775e-04\n",
      "Epoch 77/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.8239e-04\n",
      "Epoch 78/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.9058e-04\n",
      "Epoch 79/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.8736e-04\n",
      "Epoch 80/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.0030e-04\n",
      "Epoch 81/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.7957e-04\n",
      "Epoch 82/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.7057e-04\n",
      "Epoch 83/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.3870e-04\n",
      "Epoch 84/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.5036e-04\n",
      "Epoch 85/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.5983e-04\n",
      "Epoch 86/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 2.8041e-04\n",
      "Epoch 87/110\n",
      "2878/2878 [==============================] - 8s 3ms/step - loss: 3.6914e-04\n",
      "Epoch 88/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.4020e-04\n",
      "Epoch 89/110\n",
      "2878/2878 [==============================] - 7s 3ms/step - loss: 3.9847e-04\n",
      "Epoch 90/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.1743e-04\n",
      "Epoch 91/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.5047e-04\n",
      "Epoch 92/110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.4738e-04\n",
      "Epoch 93/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.8404e-04\n",
      "Epoch 94/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.8801e-04\n",
      "Epoch 95/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.4358e-04\n",
      "Epoch 96/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.3779e-04\n",
      "Epoch 97/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.3884e-04\n",
      "Epoch 98/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 2.9577e-04\n",
      "Epoch 99/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.7515e-04\n",
      "Epoch 100/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 4.0712e-04\n",
      "Epoch 101/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.2451e-04\n",
      "Epoch 102/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.5138e-04\n",
      "Epoch 103/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.6044e-04\n",
      "Epoch 104/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.1665e-04\n",
      "Epoch 105/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.1110e-04\n",
      "Epoch 106/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.0340e-04\n",
      "Epoch 107/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 2.7221e-04\n",
      "Epoch 108/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.1292e-04\n",
      "Epoch 109/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.8369e-04\n",
      "Epoch 110/110\n",
      "2878/2878 [==============================] - 7s 2ms/step - loss: 3.5173e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1caf7116358>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Compiling the RNN')\n",
    "#Adam performs relevant updates for the weights making it easy for the models optimization\n",
    "#loss is mean squared error for its continuity (ERROR IS MEAN BETWEEN OF SQUARED DIFFERENCES BETWEEN TARGETS AND ACTUAL VALUES)\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "print('Fitting the RNN to the Training set')\n",
    "print('Training The Data')\n",
    "\n",
    "regressor.fit(X_train_btc, y_train_btc, batch_size = 48, epochs = 110)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOSS IS MODERATELY LOW SO THE TRAINING DATA SO THIS GIVES A POSSIBLITY OF THE TESTING DATA GATHERING ACCURATE RESULTS AND NOT SHOW OVERFITTING OR UNDERFITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T22:27:24.632437Z",
     "start_time": "2018-09-08T22:27:24.622437Z"
    }
   },
   "outputs": [],
   "source": [
    "#GENERATING/VISUALIZING PREDICTIONS\n",
    "#GETTING THE REAL STOCK PRICE OF BITCOIN Aug8,2018-9/82018\n",
    "import matplotlib.pyplot as plt\n",
    "btc_test_set = pd.read_csv('BTC-USD_test.csv')\n",
    "\n",
    "real_stock_price = btc_test_set.iloc[:,1:2].values\n",
    "dataset_total = pd.concat((btc_train_set['Open'],btc_test_set['Open']),axis=0)\n",
    "\n",
    "# Getting the predicted stock price of Aug8,2018-9/82018\n",
    "inputs = dataset_total[len(dataset_total)- len(btc_test_set) -60:].values\n",
    "inputs = np.reshape(inputs, (-1,1))\n",
    "inputs = sc.transform(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T22:27:26.974697Z",
     "start_time": "2018-09-08T22:27:25.917564Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_btc = []\n",
    "for i in range(60,80):\n",
    "    X_test_btc.append(inputs[i-60:i, 0])\n",
    "X_test_btc = np.array(X_test_btc)\n",
    "X_test_btc = np.reshape(X_test_btc,(X_test_btc.shape[0],X_train_btc.shape[1],1))\n",
    "predicted_stock_price = regressor.predict(X_test_btc)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T22:27:28.920794Z",
     "start_time": "2018-09-08T22:27:28.799091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t real stock price 2017 \n",
      " [[7736.25    ]\n",
      " [7610.899902]\n",
      " [7542.339844]\n",
      " [7417.600098]\n",
      " [7017.890137]\n",
      " [7042.569824]\n",
      " [6945.77002 ]\n",
      " [6723.290039]\n",
      " [6285.060059]\n",
      " [6543.25    ]\n",
      " [6152.950195]\n",
      " [6091.140137]\n",
      " [6322.410156]\n",
      " [6263.200195]\n",
      " [6199.629883]\n",
      " [6274.220215]\n",
      " [6323.810059]\n",
      " [6591.180176]\n",
      " [6405.740234]\n",
      " [6502.240234]\n",
      " [6270.089844]\n",
      " [6491.089844]\n",
      " [6366.140137]\n",
      " [6539.129883]\n",
      " [6708.939941]\n",
      " [6749.709961]\n",
      " [6720.600098]\n",
      " [6915.919922]\n",
      " [7091.379883]\n",
      " [7051.609863]\n",
      " [6998.759766]\n",
      " [7026.959961]\n",
      " [7203.459961]\n",
      " [7301.25    ]\n",
      " [7270.049805]\n",
      " [7369.890137]\n",
      " [6705.029785]]\n",
      "\n",
      " predicted stock price 2017 [[8280.711 ]\n",
      " [8114.773 ]\n",
      " [7985.8267]\n",
      " [7890.7676]\n",
      " [7782.202 ]\n",
      " [7513.198 ]\n",
      " [7360.6035]\n",
      " [7260.0215]\n",
      " [7102.08  ]\n",
      " [6784.9043]\n",
      " [6719.2   ]\n",
      " [6546.8315]\n",
      " [6397.2344]\n",
      " [6446.659 ]\n",
      " [6489.9604]\n",
      " [6475.068 ]\n",
      " [6494.9634]\n",
      " [6542.155 ]\n",
      " [6713.9414]\n",
      " [6731.617 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('\\t real stock price 2017 \\n', real_stock_price)\n",
    "# Visualising the results\n",
    "plt.plot(real_stock_price, color = 'red', label = 'Real Google Stock Price')\n",
    "print('\\n predicted stock price 2017',predicted_stock_price)\n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Google Stock Price')\n",
    "plt.title('BITCOIN Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('BITCOIN Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SINCE THE DATA IS HIGHLY NON-LINEAR AND FAST CHANGING, IT IS OKAY IF THE PREDICTIONS ARE SOMEWHAT LACKING BEHIND THE REAL PREDICTIONS. IF CHANGES ARE SMOOTH, THIS MEANS THAT THE MODEL CAPTURED THE UPWARD AND DOWNWARD AND STABLE TRENDS IN THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T22:29:57.610161Z",
     "start_time": "2018-09-08T22:29:57.605147Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437.84264426688696"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(real_stock_price[:20], predicted_stock_price))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-03T20:58:12.873811Z",
     "start_time": "2018-09-03T20:58:08.811642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/transpose_1:0\", shape=(?, 12, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "n_inputs = 5\n",
    "inputs = {'btc':0,\n",
    "         'bch':1,\n",
    "         'eth':21,\n",
    "         'ltc':3,\n",
    "         'xrp':4}\n",
    "n_hidden = 2\n",
    "n_outputs = 1\n",
    "n_steps = 12\n",
    "output = {'Buy':[btc,bch,eth,ltc,xrp],\n",
    "          'Sell':[btc,bch,eth,ltc,xrp],\n",
    "          'Hold':[btc,bch,eth,ltc,xrp]}\n",
    "\n",
    "X0 = tf.placeholder(tf.float32,[None, n_steps,n_inputs], name='X0')\n",
    "X1 = tf.placeholder(tf.float32,[None, n_steps,n_inputs], name='X1')\n",
    "X2 = tf.placeholder(tf.float32,[None, n_steps,n_inputs], name='X2')\n",
    "X3 = tf.placeholder(tf.float32,[None, n_steps,n_inputs], name='X3')\n",
    "X4 = tf.placeholder(tf.float32,[None, n_steps,n_inputs], name='X4')\n",
    "\n",
    "Wx = tf.Variable(tf.random_normal(shape=[n_inputs,n_hidden],dtype=tf.float32))\n",
    "Wy = tf.Variable(tf.random_normal(shape=[n_hidden,n_hidden],dtype=tf.float32))\n",
    "\n",
    "b = tf.Variable(tf.zeros([1,n_hidden],dtype=tf.float32))\n",
    "\n",
    "X_seqs = tf.unstack(tf.transpose(X0,perm=[1,0,2])) \n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_hidden)\n",
    "outputs,states = tf.nn.dynamic_rnn(basic_cell,X0,dtype=tf.float32)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-03T20:28:38.024727Z",
     "start_time": "2018-09-03T20:28:38.009769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1)\n",
      "Tensor(\"logistic_loss:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y = 1. - tf.to_float(action)\n",
    "print(y.shape)\n",
    "learning_rate = .01\n",
    "threshold = 1.\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "print(xentropy)\n",
    "optim = tf.train.AdamOptimizer(learning_rate)\n",
    "# optim = tf.train.MomentumOptimizer(learning_rate,momentum=.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-03T20:28:39.477571Z",
     "start_time": "2018-09-03T20:28:39.162415Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-bdaa1f24ede0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mgradient_placeholder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mgradient_placeholders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_placeholder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mgrads_and_vars_feed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_placeholder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_shape'"
     ]
    }
   ],
   "source": [
    "grads_and_vars = optim.compute_gradients(xentropy)\n",
    "gradients = [grad for grad, variable in grads_and_vars]\n",
    "gradient_placeholders = []\n",
    "grads_and_vars_feed = []\n",
    "\n",
    "for grad, variable in grads_and_vars:\n",
    "    gradient_placeholder = tf.placeholder(tf.float32,shape=grad.get_shape())\n",
    "    gradient_placeholders.append(gradient_placeholder)\n",
    "    grads_and_vars_feed.append((gradient_placeholder, variable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_op = optim.apply_gradients(grads_and_vars_feed)\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def discount_rewards(rewards,discount_rate):\n",
    "    discounted_rewards = np.empty(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] +cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards,discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards,discount_rate)\n",
    "                             for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/ reward_std\n",
    "           for discounted_rewards in all_discounted_rewards]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_rewards([10,0,-50],discount_rate=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_and_normalize_rewards([[10,0,-50],[10,20]],discount_rate=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 250\n",
    "n_max_steps = 1000\n",
    "n_games_per_update = 10 \n",
    "save_iterations = 10 \n",
    "discount_rate = .95\n",
    "nan = np.nan\n",
    "T = np.array([p_charge_and_p_wait_and_p_search,action,n_outputs])\n",
    "print(T)\n",
    "R = np.array([p_charge_and_p_wait_and_p_search,action,n_outputs])\n",
    "print(R)\n",
    "\n",
    "possible_actions = [[inputs],[action],[outputs]]\n",
    "print(possible_actions)\n",
    "\n",
    "Q= np.full((3,3), - np.inf)\n",
    "for state,actions in enumerate(possible_actions):\n",
    "    Q[inputs, outputs] = 0.0\n",
    "discount_rate = .95\n",
    "n_iter = 100\n",
    "for iteration in range(n_iter):\n",
    "    Q_prev = Q.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q[s,a] = np.sum([T[s, a, outputs] * (R[s, a, outputs] +discount_rate * np.max(Q_prev[outputs]))\n",
    "                             for outputs in range(3)])\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T05:09:48.786980Z",
     "start_time": "2018-09-02T05:09:48.426944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 72)                21312     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 64)                4672      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 12)                396       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 4)                 52        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 28,525\n",
      "Trainable params: 28,525\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the input layer and the LSTM layer\n",
    "regressor.add(LSTM(units = 72, activation = 'sigmoid', input_shape = (None, 1)))\n",
    "regressor.add(Dense(units= 64, activation = 'relu'))\n",
    "regressor.add(Dense(units=32, activation= 'softmax'))\n",
    "regressor.add(Dense(units=12, activation= 'relu'))\n",
    "regressor.add(Dropout(.25))\n",
    "regressor.add(Dense(units=4, activation= 'relu'))\n",
    "regressor.add(Dense(units=2, activation= 'sigmoid'))\n",
    "\n",
    "\n",
    "# regressor.add(Dense(output_dim = 164, activation = 'relu'))\n",
    "# regressor.add(Dense(units= 212, activation = 'softmax'))\n",
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "regressor.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T05:09:56.270281Z",
     "start_time": "2018-09-02T05:09:48.788986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the RNN\n",
      "Fitting the RNN to the Training set\n",
      "Training The Data\n",
      "Epoch 1/200\n",
      "1174/1174 [==============================] - 1s 935us/step - loss: 0.0019 - acc: 8.5179e-04\n",
      "Epoch 2/200\n",
      "1174/1174 [==============================] - 0s 26us/step - loss: 7.4273e-04 - acc: 8.5179e-04\n",
      "Epoch 3/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.4544e-04 - acc: 8.5179e-04\n",
      "Epoch 4/200\n",
      "1174/1174 [==============================] - 0s 26us/step - loss: 3.3076e-04 - acc: 8.5179e-04\n",
      "Epoch 5/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.4934e-04 - acc: 8.5179e-04\n",
      "Epoch 6/200\n",
      "1174/1174 [==============================] - 0s 24us/step - loss: 3.2483e-04 - acc: 8.5179e-04\n",
      "Epoch 7/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.1096e-04 - acc: 8.5179e-04\n",
      "Epoch 8/200\n",
      "1174/1174 [==============================] - 0s 34us/step - loss: 3.1036e-04 - acc: 8.5179e-04\n",
      "Epoch 9/200\n",
      "1174/1174 [==============================] - 0s 25us/step - loss: 3.0972e-04 - acc: 8.5179e-04\n",
      "Epoch 10/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0839e-04 - acc: 8.5179e-04\n",
      "Epoch 11/200\n",
      "1174/1174 [==============================] - 0s 25us/step - loss: 3.0846e-04 - acc: 8.5179e-04\n",
      "Epoch 12/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0870e-04 - acc: 8.5179e-04\n",
      "Epoch 13/200\n",
      "1174/1174 [==============================] - 0s 25us/step - loss: 3.0890e-04 - acc: 8.5179e-04\n",
      "Epoch 14/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0869e-04 - acc: 8.5179e-04\n",
      "Epoch 15/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0868e-04 - acc: 8.5179e-04\n",
      "Epoch 16/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0846e-04 - acc: 8.5179e-04\n",
      "Epoch 17/200\n",
      "1174/1174 [==============================] - 0s 25us/step - loss: 3.0856e-04 - acc: 8.5179e-04\n",
      "Epoch 18/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0877e-04 - acc: 8.5179e-04\n",
      "Epoch 19/200\n",
      "1174/1174 [==============================] - 0s 28us/step - loss: 3.0853e-04 - acc: 8.5179e-04\n",
      "Epoch 20/200\n",
      "1174/1174 [==============================] - 0s 25us/step - loss: 3.0799e-04 - acc: 8.5179e-04\n",
      "Epoch 21/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0823e-04 - acc: 8.5179e-04\n",
      "Epoch 22/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0853e-04 - acc: 8.5179e-04\n",
      "Epoch 23/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0843e-04 - acc: 8.5179e-04\n",
      "Epoch 24/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0852e-04 - acc: 8.5179e-04\n",
      "Epoch 25/200\n",
      "1174/1174 [==============================] - 0s 24us/step - loss: 3.0845e-04 - acc: 8.5179e-04\n",
      "Epoch 26/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0808e-04 - acc: 8.5179e-04\n",
      "Epoch 27/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0819e-04 - acc: 8.5179e-04\n",
      "Epoch 28/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0813e-04 - acc: 8.5179e-04\n",
      "Epoch 29/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0863e-04 - acc: 8.5179e-04\n",
      "Epoch 30/200\n",
      "1174/1174 [==============================] - 0s 25us/step - loss: 3.0842e-04 - acc: 8.5179e-04\n",
      "Epoch 31/200\n",
      "1174/1174 [==============================] - 0s 24us/step - loss: 3.0869e-04 - acc: 8.5179e-04\n",
      "Epoch 32/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0863e-04 - acc: 8.5179e-04\n",
      "Epoch 33/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0831e-04 - acc: 8.5179e-04\n",
      "Epoch 34/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0843e-04 - acc: 8.5179e-04\n",
      "Epoch 35/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0827e-04 - acc: 8.5179e-04\n",
      "Epoch 36/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0825e-04 - acc: 8.5179e-04\n",
      "Epoch 37/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0857e-04 - acc: 8.5179e-04\n",
      "Epoch 38/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0831e-04 - acc: 8.5179e-04\n",
      "Epoch 39/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0831e-04 - acc: 8.5179e-04\n",
      "Epoch 40/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0862e-04 - acc: 8.5179e-04\n",
      "Epoch 41/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0834e-04 - acc: 8.5179e-04\n",
      "Epoch 42/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0836e-04 - acc: 8.5179e-04\n",
      "Epoch 43/200\n",
      "1174/1174 [==============================] - 0s 25us/step - loss: 3.0889e-04 - acc: 8.5179e-04\n",
      "Epoch 44/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0841e-04 - acc: 8.5179e-04\n",
      "Epoch 45/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0821e-04 - acc: 8.5179e-04\n",
      "Epoch 46/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0853e-04 - acc: 8.5179e-04\n",
      "Epoch 47/200\n",
      "1174/1174 [==============================] - 0s 24us/step - loss: 3.0900e-04 - acc: 8.5179e-04\n",
      "Epoch 48/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0837e-04 - acc: 8.5179e-04\n",
      "Epoch 49/200\n",
      "1174/1174 [==============================] - 0s 28us/step - loss: 3.0909e-04 - acc: 8.5179e-04\n",
      "Epoch 50/200\n",
      "1174/1174 [==============================] - 0s 25us/step - loss: 3.0908e-04 - acc: 8.5179e-04\n",
      "Epoch 51/200\n",
      "1174/1174 [==============================] - 0s 24us/step - loss: 3.0869e-04 - acc: 8.5179e-04\n",
      "Epoch 52/200\n",
      "1174/1174 [==============================] - 0s 33us/step - loss: 3.0889e-04 - acc: 8.5179e-04\n",
      "Epoch 53/200\n",
      "1174/1174 [==============================] - 0s 25us/step - loss: 3.0905e-04 - acc: 8.5179e-04\n",
      "Epoch 54/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0831e-04 - acc: 8.5179e-04\n",
      "Epoch 55/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0869e-04 - acc: 8.5179e-04\n",
      "Epoch 56/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0812e-04 - acc: 8.5179e-04\n",
      "Epoch 57/200\n",
      "1174/1174 [==============================] - 0s 19us/step - loss: 3.0857e-04 - acc: 8.5179e-04\n",
      "Epoch 58/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0839e-04 - acc: 8.5179e-04\n",
      "Epoch 59/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0832e-04 - acc: 8.5179e-04\n",
      "Epoch 60/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0853e-04 - acc: 8.5179e-04\n",
      "Epoch 61/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0836e-04 - acc: 8.5179e-04\n",
      "Epoch 62/200\n",
      "1174/1174 [==============================] - 0s 24us/step - loss: 3.0900e-04 - acc: 8.5179e-04\n",
      "Epoch 63/200\n",
      "1174/1174 [==============================] - 0s 25us/step - loss: 3.0852e-04 - acc: 8.5179e-04\n",
      "Epoch 64/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0840e-04 - acc: 8.5179e-04\n",
      "Epoch 65/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0842e-04 - acc: 8.5179e-04\n",
      "Epoch 66/200\n",
      "1174/1174 [==============================] - 0s 24us/step - loss: 3.0906e-04 - acc: 8.5179e-04\n",
      "Epoch 67/200\n",
      "1174/1174 [==============================] - 0s 24us/step - loss: 3.0853e-04 - acc: 8.5179e-04\n",
      "Epoch 68/200\n",
      "1174/1174 [==============================] - 0s 27us/step - loss: 3.0927e-04 - acc: 8.5179e-04\n",
      "Epoch 69/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.1046e-04 - acc: 8.5179e-04\n",
      "Epoch 70/200\n",
      "1174/1174 [==============================] - 0s 25us/step - loss: 3.0845e-04 - acc: 8.5179e-04\n",
      "Epoch 71/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0850e-04 - acc: 8.5179e-04\n",
      "Epoch 72/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0835e-04 - acc: 8.5179e-04\n",
      "Epoch 73/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0862e-04 - acc: 8.5179e-04\n",
      "Epoch 74/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0904e-04 - acc: 8.5179e-04\n",
      "Epoch 75/200\n",
      "1174/1174 [==============================] - 0s 25us/step - loss: 3.0833e-04 - acc: 8.5179e-04\n",
      "Epoch 76/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0841e-04 - acc: 8.5179e-04\n",
      "Epoch 77/200\n",
      "1174/1174 [==============================] - 0s 24us/step - loss: 3.0834e-04 - acc: 8.5179e-04\n",
      "Epoch 78/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0844e-04 - acc: 8.5179e-04\n",
      "Epoch 79/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0840e-04 - acc: 8.5179e-04\n",
      "Epoch 80/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0837e-04 - acc: 8.5179e-04\n",
      "Epoch 81/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0829e-04 - acc: 8.5179e-04\n",
      "Epoch 82/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0842e-04 - acc: 8.5179e-04\n",
      "Epoch 83/200\n",
      "1174/1174 [==============================] - 0s 29us/step - loss: 3.0854e-04 - acc: 8.5179e-04\n",
      "Epoch 84/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0833e-04 - acc: 8.5179e-04\n",
      "Epoch 85/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0862e-04 - acc: 8.5179e-04\n",
      "Epoch 86/200\n",
      "1174/1174 [==============================] - 0s 24us/step - loss: 3.0900e-04 - acc: 8.5179e-04\n",
      "Epoch 87/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0857e-04 - acc: 8.5179e-04\n",
      "Epoch 88/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0927e-04 - acc: 8.5179e-04\n",
      "Epoch 89/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0878e-04 - acc: 8.5179e-04\n",
      "Epoch 90/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0923e-04 - acc: 8.5179e-04\n",
      "Epoch 91/200\n",
      "1174/1174 [==============================] - 0s 28us/step - loss: 3.0857e-04 - acc: 8.5179e-04\n",
      "Epoch 92/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0856e-04 - acc: 8.5179e-04\n",
      "Epoch 93/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0917e-04 - acc: 8.5179e-04\n",
      "Epoch 94/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0887e-04 - acc: 8.5179e-04\n",
      "Epoch 95/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0846e-04 - acc: 8.5179e-04\n",
      "Epoch 96/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0847e-04 - acc: 8.5179e-04\n",
      "Epoch 97/200\n",
      "1174/1174 [==============================] - 0s 32us/step - loss: 3.0869e-04 - acc: 8.5179e-04\n",
      "Epoch 98/200\n",
      "1174/1174 [==============================] - 0s 25us/step - loss: 3.0829e-04 - acc: 8.5179e-04\n",
      "Epoch 99/200\n",
      "1174/1174 [==============================] - 0s 25us/step - loss: 3.0897e-04 - acc: 8.5179e-04\n",
      "Epoch 100/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0920e-04 - acc: 8.5179e-04\n",
      "Epoch 101/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0822e-04 - acc: 8.5179e-04\n",
      "Epoch 102/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0877e-04 - acc: 8.5179e-04\n",
      "Epoch 103/200\n",
      "1174/1174 [==============================] - 0s 25us/step - loss: 3.0869e-04 - acc: 8.5179e-04\n",
      "Epoch 104/200\n",
      "1174/1174 [==============================] - 0s 25us/step - loss: 3.0849e-04 - acc: 8.5179e-04\n",
      "Epoch 105/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0840e-04 - acc: 8.5179e-04\n",
      "Epoch 106/200\n",
      "1174/1174 [==============================] - 0s 24us/step - loss: 3.0842e-04 - acc: 8.5179e-04\n",
      "Epoch 107/200\n",
      "1174/1174 [==============================] - 0s 26us/step - loss: 3.0854e-04 - acc: 8.5179e-04\n",
      "Epoch 108/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0916e-04 - acc: 8.5179e-04\n",
      "Epoch 109/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0875e-04 - acc: 8.5179e-04\n",
      "Epoch 110/200\n",
      "1174/1174 [==============================] - 0s 24us/step - loss: 3.0850e-04 - acc: 8.5179e-04\n",
      "Epoch 111/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0826e-04 - acc: 8.5179e-04\n",
      "Epoch 112/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0852e-04 - acc: 8.5179e-04\n",
      "Epoch 113/200\n",
      "1174/1174 [==============================] - 0s 24us/step - loss: 3.0916e-04 - acc: 8.5179e-04\n",
      "Epoch 114/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0849e-04 - acc: 8.5179e-04\n",
      "Epoch 115/200\n",
      "1174/1174 [==============================] - 0s 19us/step - loss: 3.0878e-04 - acc: 8.5179e-04\n",
      "Epoch 116/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0866e-04 - acc: 8.5179e-04\n",
      "Epoch 117/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0908e-04 - acc: 8.5179e-04\n",
      "Epoch 118/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0859e-04 - acc: 8.5179e-04\n",
      "Epoch 119/200\n",
      "1174/1174 [==============================] - 0s 19us/step - loss: 3.0854e-04 - acc: 8.5179e-04\n",
      "Epoch 120/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0858e-04 - acc: 8.5179e-04\n",
      "Epoch 121/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0876e-04 - acc: 8.5179e-04\n",
      "Epoch 122/200\n",
      "1174/1174 [==============================] - 0s 26us/step - loss: 3.0948e-04 - acc: 8.5179e-04\n",
      "Epoch 123/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0886e-04 - acc: 8.5179e-04\n",
      "Epoch 124/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0860e-04 - acc: 8.5179e-04\n",
      "Epoch 125/200\n",
      "1174/1174 [==============================] - 0s 24us/step - loss: 3.0853e-04 - acc: 8.5179e-04\n",
      "Epoch 126/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0861e-04 - acc: 8.5179e-04\n",
      "Epoch 127/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0802e-04 - acc: 8.5179e-04\n",
      "Epoch 128/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0948e-04 - acc: 8.5179e-04\n",
      "Epoch 129/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0870e-04 - acc: 8.5179e-04\n",
      "Epoch 130/200\n",
      "1174/1174 [==============================] - 0s 25us/step - loss: 3.0864e-04 - acc: 8.5179e-04\n",
      "Epoch 131/200\n",
      "1174/1174 [==============================] - 0s 24us/step - loss: 3.1092e-04 - acc: 8.5179e-04\n",
      "Epoch 132/200\n",
      "1174/1174 [==============================] - 0s 19us/step - loss: 3.0900e-04 - acc: 8.5179e-04\n",
      "Epoch 133/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0948e-04 - acc: 8.5179e-04\n",
      "Epoch 134/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0820e-04 - acc: 8.5179e-04\n",
      "Epoch 135/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0881e-04 - acc: 8.5179e-04\n",
      "Epoch 136/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0833e-04 - acc: 8.5179e-04\n",
      "Epoch 137/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0866e-04 - acc: 8.5179e-04\n",
      "Epoch 138/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0870e-04 - acc: 8.5179e-04\n",
      "Epoch 139/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0881e-04 - acc: 8.5179e-04\n",
      "Epoch 140/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0875e-04 - acc: 8.5179e-04\n",
      "Epoch 141/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0852e-04 - acc: 8.5179e-04\n",
      "Epoch 142/200\n",
      "1174/1174 [==============================] - 0s 27us/step - loss: 3.0931e-04 - acc: 8.5179e-04\n",
      "Epoch 143/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.1062e-04 - acc: 8.5179e-04\n",
      "Epoch 144/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0852e-04 - acc: 8.5179e-04\n",
      "Epoch 145/200\n",
      "1174/1174 [==============================] - 0s 19us/step - loss: 3.0928e-04 - acc: 8.5179e-04\n",
      "Epoch 146/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0942e-04 - acc: 8.5179e-04\n",
      "Epoch 147/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.1009e-04 - acc: 8.5179e-04\n",
      "Epoch 148/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0948e-04 - acc: 8.5179e-04\n",
      "Epoch 149/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0863e-04 - acc: 8.5179e-04\n",
      "Epoch 150/200\n",
      "1174/1174 [==============================] - 0s 19us/step - loss: 3.0929e-04 - acc: 8.5179e-04\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0872e-04 - acc: 8.5179e-04\n",
      "Epoch 152/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0870e-04 - acc: 8.5179e-04\n",
      "Epoch 153/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0839e-04 - acc: 8.5179e-04\n",
      "Epoch 154/200\n",
      "1174/1174 [==============================] - 0s 18us/step - loss: 3.0913e-04 - acc: 8.5179e-04\n",
      "Epoch 155/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0797e-04 - acc: 8.5179e-04\n",
      "Epoch 156/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0921e-04 - acc: 8.5179e-04\n",
      "Epoch 157/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0853e-04 - acc: 8.5179e-04\n",
      "Epoch 158/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0821e-04 - acc: 8.5179e-04\n",
      "Epoch 159/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0868e-04 - acc: 8.5179e-04\n",
      "Epoch 160/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0928e-04 - acc: 8.5179e-04\n",
      "Epoch 161/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0877e-04 - acc: 8.5179e-04\n",
      "Epoch 162/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0832e-04 - acc: 8.5179e-04\n",
      "Epoch 163/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0840e-04 - acc: 8.5179e-04\n",
      "Epoch 164/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0850e-04 - acc: 8.5179e-04\n",
      "Epoch 165/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0862e-04 - acc: 8.5179e-04\n",
      "Epoch 166/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0836e-04 - acc: 8.5179e-04\n",
      "Epoch 167/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0988e-04 - acc: 8.5179e-04\n",
      "Epoch 168/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0865e-04 - acc: 8.5179e-04\n",
      "Epoch 169/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0823e-04 - acc: 8.5179e-04\n",
      "Epoch 170/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0846e-04 - acc: 8.5179e-04\n",
      "Epoch 171/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0867e-04 - acc: 8.5179e-04\n",
      "Epoch 172/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0858e-04 - acc: 8.5179e-04\n",
      "Epoch 173/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0847e-04 - acc: 8.5179e-04\n",
      "Epoch 174/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0828e-04 - acc: 8.5179e-04\n",
      "Epoch 175/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0877e-04 - acc: 8.5179e-04\n",
      "Epoch 176/200\n",
      "1174/1174 [==============================] - 0s 18us/step - loss: 3.0850e-04 - acc: 8.5179e-04\n",
      "Epoch 177/200\n",
      "1174/1174 [==============================] - 0s 25us/step - loss: 3.0894e-04 - acc: 8.5179e-04\n",
      "Epoch 178/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0838e-04 - acc: 8.5179e-04\n",
      "Epoch 179/200\n",
      "1174/1174 [==============================] - 0s 24us/step - loss: 3.0831e-04 - acc: 8.5179e-04\n",
      "Epoch 180/200\n",
      "1174/1174 [==============================] - ETA: 0s - loss: 2.9790e-04 - acc: 0.0000e+0 - 0s 20us/step - loss: 3.0892e-04 - acc: 8.5179e-04\n",
      "Epoch 181/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0839e-04 - acc: 8.5179e-04\n",
      "Epoch 182/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0845e-04 - acc: 8.5179e-04\n",
      "Epoch 183/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0960e-04 - acc: 8.5179e-04\n",
      "Epoch 184/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.1112e-04 - acc: 8.5179e-04\n",
      "Epoch 185/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0836e-04 - acc: 8.5179e-04\n",
      "Epoch 186/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0860e-04 - acc: 8.5179e-04\n",
      "Epoch 187/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0872e-04 - acc: 8.5179e-04\n",
      "Epoch 188/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0846e-04 - acc: 8.5179e-04\n",
      "Epoch 189/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0880e-04 - acc: 8.5179e-04\n",
      "Epoch 190/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0856e-04 - acc: 8.5179e-04\n",
      "Epoch 191/200\n",
      "1174/1174 [==============================] - 0s 26us/step - loss: 3.0875e-04 - acc: 8.5179e-04\n",
      "Epoch 192/200\n",
      "1174/1174 [==============================] - 0s 23us/step - loss: 3.0889e-04 - acc: 8.5179e-04\n",
      "Epoch 193/200\n",
      "1174/1174 [==============================] - 0s 24us/step - loss: 3.0824e-04 - acc: 8.5179e-04\n",
      "Epoch 194/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.0862e-04 - acc: 8.5179e-04\n",
      "Epoch 195/200\n",
      "1174/1174 [==============================] - 0s 22us/step - loss: 3.0852e-04 - acc: 8.5179e-04\n",
      "Epoch 196/200\n",
      "1174/1174 [==============================] - 0s 24us/step - loss: 3.0992e-04 - acc: 8.5179e-04\n",
      "Epoch 197/200\n",
      "1174/1174 [==============================] - 0s 25us/step - loss: 3.0977e-04 - acc: 8.5179e-04\n",
      "Epoch 198/200\n",
      "1174/1174 [==============================] - 0s 19us/step - loss: 3.1121e-04 - acc: 8.5179e-04\n",
      "Epoch 199/200\n",
      "1174/1174 [==============================] - 0s 21us/step - loss: 3.1009e-04 - acc: 8.5179e-04\n",
      "Epoch 200/200\n",
      "1174/1174 [==============================] - 0s 20us/step - loss: 3.0870e-04 - acc: 8.5179e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b217762160>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Compiling the RNN')\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics= ['accuracy'])\n",
    "print('Fitting the RNN to the Training set')\n",
    "print('Training The Data')\n",
    "regressor.fit(X_train_ltc, y_train_ltc, batch_size = 156, epochs = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
